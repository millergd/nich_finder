{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Merch Data Shirts\n",
    "1. read in data file and create large list of indexed data\n",
    "2. remove texts that pop up in all documents (i.e. stopwords / phrases)\n",
    "3. clean the documents\n",
    "4. stem and tokenize the words in each document\n",
    "5. run all documents through tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- read in datafiles better (use different delimiter?)\n",
    "- clean up input documents better\n",
    "- possibly create my own tfidf vectorizor that works as a sum rather than a median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the stemmer and then tokenize and stem the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artwork']\n"
     ]
    }
   ],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if not re.search('[^a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation) NOTE: this might be bad\n",
    "    for token in tokens:\n",
    "        if not re.search('[^a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "print(tokenize_and_stem('artwork'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the shirt data file and create documents from it. Each document is the title of the shirt followed by it's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'salesRank': 'NA', 'asin': 'B07X97D31R', 'title': 'Support Cancer Shirt Prostate Cancer Awareness Tshirt', 'imgUrl': 'https://m.media-amazon.com/images/I/A13usaonutL._CLa%7C2140%2C2000%7C918G0EUHpmL.png%7C0%2C0%2C2140%2C2000%2B0.0%2C0.0%2C2140.0%2C2000.0._UX342_.png', 'trademarked': 'False', 'isMerch': 'True', 'date': 'August 30, 2019', 'unix': '1567123200', 'errorMessage': '[]', 'link': 'https://www.amazon.com/dp/B07X97D31R', 'description': 'Solid colors: 100% Cotton; Heather Grey: 90% Cotton, 10% Polyester; All Other Heathers: 50% Cotton, 50% Polyester Imported Machine wash cold with like colors, dry low heat Are you a Fighter who has or is fighting Prostate Cancer? This Motivational T-shirt is perfect for you. Great shirt for your Light Blue ribbon events, Hospital Visits or To support a loved one with Cancer. This Prostate Cancer Tshirt is a great Birthday or Christmas Gift For Surviors. Show your love with a Light Blue Ribbon. Lightweight, Classic fit, Double-needle sleeve and bottom hem\\n', 'document': 'support cancer shirt prostate cancer awareness tshirt'}\n",
      "16010\n"
     ]
    }
   ],
   "source": [
    "def clean_string(document):\n",
    "    document = document.lower()\n",
    "    default_1 = \"Lightweight, Classic fit, Double-needle sleeve and bottom hem\".lower()\n",
    "    default_2 = \"Solid colors: 100% Cotton; Heather Grey: 90% Cotton, 10% Polyester; All Other Heathers: 50% Cotton, 50% Polyester Imported Machine wash cold with like colors, dry low heat \".lower()\n",
    "    document = document.replace(default_1, '').replace(default_2,'')\n",
    "    \n",
    "    document.replace(\"tee shirt\",\"tshirt\")\n",
    "    document.replace(\"t-shirt\", 'tshirt')\n",
    "    document.replace(' t shirt', 'tshirt')\n",
    "    document.replace(\"-\", \" \")\n",
    "    \n",
    "    document.replace(\"shirt\", \"\")\n",
    "    document.replace(\"tshirt\", \"\")\n",
    "    \n",
    "    document.replace(\"officially licensed\",\"\")\n",
    "    \n",
    "    if re.compile(\"officially licensed [\\w\\s]+ (apparel|shirt)\").search(document):\n",
    "        document = re.sub(r'officially licensed [\\w\\s]+ (apparel|shirt)', '', document)\n",
    "    \n",
    "    if re.compile(\"official [\\w\\s]+merchandise\").search(document):\n",
    "        document = re.sub(r'official [\\w\\s]+merchandise', '', document)\n",
    "\n",
    "    if re.compile(\"graphic [\\w\\s\\-]+shirt\").search(document):\n",
    "        document = re.sub(r'graphic [\\w\\s\\-]+shirt', '', document)\n",
    "    \n",
    "    \n",
    "    document = BeautifulSoup(document, 'html.parser').getText()\n",
    "    \n",
    "    return document\n",
    "\n",
    "document_data_dict = {}\n",
    "def read_shirt_data_file(path):\n",
    "    document_data = []\n",
    "    with open(path, 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = {}\n",
    "            if len(line.replace(\"\\\"\",\"\").split('|')) == 11:\n",
    "                for item in line.replace(\"\\\"\",\"\").split('|'):\n",
    "                    data[item.split(':',1)[0]] = item.split(':',1)[1]\n",
    "                data['document'] = clean_string(data['title']) # + \". \" + data['description'])\n",
    "\n",
    "                # Keep track of document data by asin\n",
    "                if data['asin'] not in document_data_dict:\n",
    "                    document_data_dict[data['asin']] = data\n",
    "                    document_data.append(data)\n",
    "                \n",
    "    return document_data\n",
    "\n",
    "document_data = read_shirt_data_file(\"shirts_newest_nt\") # read_shirt_data_file(\"shirts_featured_nt\")\n",
    "print(document_data[0])\n",
    "print(len(document_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: \n",
    "cosine similarity of stems and ngrams (Tf-idf and document similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfidf vector from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['logo', 'shirt', 'tshirt', 'vintag', 'retro', 'movie', 'officially', 'licensed', 'official', 'offici', 'licens', 'graphic', 'artwork', 'gift', 'men', 'women', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/niche_finder/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'licen', 'mani', 'meanwhil', 'moreov', 'movi', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.65 s, sys: 42.7 ms, total: 4.7 s\n",
      "Wall time: 4.69 s\n",
      "(16010, 74640)\n",
      "74640\n",
      "adjust\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "with open('stop_words.txt','r') as f:\n",
    "    extra_stop_words = f.read().split('\\n')\n",
    "\n",
    "print(extra_stop_words)\n",
    "\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(extra_stop_words)\n",
    "#print(my_stop_words)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.02, max_features=10000000,\n",
    "                                 min_df=0, stop_words=my_stop_words,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform([document['document'] for document in document_data])\n",
    "#print(tfidf_matrix[0])\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(len(terms))\n",
    "#print(stemmed_documents[99])\n",
    "\n",
    "print(terms[247])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "checking B07X97D31R for similarities...\n",
      "support cancer shirt prostate cancer awareness tshirt\n",
      "1.0 support cancer shirt prostate cancer awareness tshirt\n",
      "0.22020866476987808 breast cancer awareness support tee\n",
      "0.2165820341351916 strength against cancer vintage childhood cancer awareness t-shirt\n",
      "0.21020493217342937 prostate cancer sucks dinosaur trex blue ribbon awareness\n",
      "0.2140131736540844 breast cancer awareness shirt breast cancer shirts for women t-shirt\n",
      "0.3377413328599886 i wear light blue for my dad prostate cancer awareness shirt\n",
      "0.35034916405125105 his fight is my fight i prostate cancer awareness fight gift t-shirt\n",
      "0.21563231640052763 fuck cancer tshirt awareness for cancer survivor gifts t-shirt\n",
      "0.3755102199922954 i wear blue for my uncle prostate cancer awareness shirt\n",
      "0.42907667964294915 wolf still here still fighting prostate cancer awareness t-shirt\n"
     ]
    }
   ],
   "source": [
    "# Testing cosine similarity comparisons\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#print(cosine_similarity(tfidf_matrix))\n",
    "# print(tfidf_matrix[0])\n",
    "# print(tfidf_matrix[1])\n",
    "# print(tfidf_matrix[2])\n",
    "\n",
    "start = 0\n",
    "length = 1\n",
    "dist = cosine_similarity(tfidf_matrix[start:start+length],tfidf_matrix)\n",
    "print(dist[0])\n",
    "for doc_vector_index, doc_vector in enumerate(dist):\n",
    "    print(\"checking {} for similarities...\".format(document_data[start + doc_vector_index]['asin']))\n",
    "    print(\"{}\".format(document_data[start + doc_vector_index]['document']))\n",
    "    for similarity_index, doc_similarity in enumerate(doc_vector):\n",
    "        if doc_similarity > .2:\n",
    "            print(\"{} {}\".format(doc_similarity, document_data[similarity_index]['document']))\n",
    "        \n",
    "\n",
    "# print(tfidf_matrix.shape)\n",
    "# print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity between all doc vectors, and create niches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now analyzing 2719\n",
      "{13567, 10021, 5990, 8007, 5128, 2506, 1005, 7597, 6132, 6777, 2719}\n",
      "straight outta 2016 3rd birthday 3 years age vintage gifts t-shirt\n",
      "{1841, 13567}\n",
      "established since september 2006 straight outta 13 years old\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "niches = {}\n",
    "\n",
    "# dist = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "for doc_vector_index, doc_vector in enumerate(dist):\n",
    "#     if doc_vector_index > 100:\n",
    "#         break\n",
    "\n",
    "    niches[doc_vector_index] = {}\n",
    "    niches[doc_vector_index]['similar_docs'] = set()\n",
    "    niches[doc_vector_index]['percent_sales_ranks'] = 0.0\n",
    "    niches[doc_vector_index]['average_sales_rank'] = 0\n",
    "    niches[doc_vector_index]['hot'] = False\n",
    "    niches[doc_vector_index]['consumed'] = False\n",
    "    for similar_doc_index, similar_doc in enumerate(doc_vector):\n",
    "        if similar_doc > .25:\n",
    "            niches[doc_vector_index]['similar_docs'].add(similar_doc_index)\n",
    "            # if doc_vector_index != similarity_index and similarity_index in niches:\n",
    "                \n",
    "    # if float(len(niches[doc_vector_index].intersection(niches[similarity_index])))/float(min(len(niches[similarity_index]),len(niches[doc_vector_index]))) > 0.75\n",
    "    if int(doc_vector_index) == 2719:\n",
    "        print(\"now analyzing 2719\")\n",
    "        print(niches[doc_vector_index]['similar_docs'])\n",
    "        print(document_data[doc_vector_index]['document'])\n",
    "        print(niches[1841]['similar_docs'])\n",
    "        print(document_data[1841]['document'])\n",
    "    for similar_doc in niches[doc_vector_index]['similar_docs']:\n",
    "        if int(doc_vector_index) == 2719 and int(similar_doc) == 1841:\n",
    "            print(\"now analyzing 2749\")\n",
    "        if doc_vector_index != similar_doc and similar_doc in niches and not niches[similar_doc]['consumed']:\n",
    "\n",
    "            intersecting_docs = niches[doc_vector_index]['similar_docs'].intersection(niches[similar_doc]['similar_docs'])\n",
    "            smaller_niche = min(len(niches[similar_doc]['similar_docs']),len(niches[doc_vector_index]['similar_docs']))                                                                          \n",
    "\n",
    "            if (float(len(intersecting_docs))/float(smaller_niche)) > 0.75:\n",
    "#                 print(\"these two share more than 75% of items\")\n",
    "#                 print(float(len(niches[doc_vector_index]['similar_docs'])))\n",
    "#                 print(doc_vector_index)\n",
    "#                 print(niches[doc_vector_index]['similar_docs'])\n",
    "#                 print(document_data[doc_vector_index]['document'])\n",
    "#                 print(float(len(niches[similar_doc]['similar_docs'])))\n",
    "#                 print(similar_doc)\n",
    "#                 print(niches[similar_doc]['similar_docs'])\n",
    "#                 print(document_data[similar_doc]['document'])\n",
    "#                 for item in niches[similar_doc]:\n",
    "#                     print(item)\n",
    "#                     print(document_data[item]['document'])\n",
    "                if len(niches[doc_vector_index]['similar_docs']) >= len(niches[similar_doc]['similar_docs']):\n",
    "                    niches[doc_vector_index]['consumed'] = True\n",
    "                else:\n",
    "                    niches[similar_doc]['consumed'] = True\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "# pp.pprint(niches)\n",
    "\n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze and export niches to textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "def analyze_niche(niche):\n",
    "    total_sales_rank = 0\n",
    "    num_sales_ranks = 0\n",
    "    for document in niche['similar_docs']:\n",
    "        if document_data[document]['salesRank'] != \"NA\":\n",
    "            num_sales_ranks += 1\n",
    "            total_sales_rank += int(document_data[document]['salesRank'])\n",
    "            if 'best_sales_rank' in niche and int(document_data[document]['salesRank']) < int(niche['best_sales_rank']):\n",
    "                niche['best_sales_rank'] = document_data[document]['salesRank']\n",
    "            elif 'best_sales_rank' not in niche:\n",
    "                niche['best_sales_rank'] = document_data[document]['salesRank']\n",
    "                \n",
    "    niche['percent_sales_ranks'] = float(num_sales_ranks) / float(len(niche['similar_docs']))\n",
    "    if num_sales_ranks > 0:\n",
    "        niche['average_sales_rank'] = float(total_sales_rank) / float(num_sales_ranks)\n",
    "\n",
    "def hot_niche(niche):\n",
    "    if len(niche['similar_docs']) > 5:\n",
    "\n",
    "        if niche['percent_sales_ranks'] > 0.8 and niche['average_sales_rank'] < 1500000:\n",
    "            return True\n",
    "    \n",
    "        if niche['percent_sales_ranks'] > 0.5 and niche['average_sales_rank'] < 900000:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "num_hot_niches = 0\n",
    "for niche in niches:\n",
    "    if len(niches[niche]['similar_docs']) > 0 and niches[niche]['consumed'] is False:\n",
    "        analyze_niche(niches[niche])\n",
    "        if hot_niche(niches[niche]):\n",
    "            num_hot_niches += 1\n",
    "            niches[niche]['hot'] = True\n",
    "\n",
    "print(num_hot_niches)\n",
    "with open(\"niches.txt\", 'w') as f:\n",
    "    for niche in niches:\n",
    "        if niches[niche]['hot'] and not niches[niche]['consumed']:\n",
    "            f.write('index: ' + str(niche) + '\\n')\n",
    "            f.write(\"number of documents in cluster: \" + str(len(niches[niche]['similar_docs'])) + '\\n')\n",
    "            f.write(\"percent sales ranks: \" + str(round(niches[niche]['percent_sales_ranks'],2)) + \"\\n\")\n",
    "            f.write(\"average sales rank: \" + str(round(niches[niche]['average_sales_rank'],2)) + \"\\n\")\n",
    "            f.write(\"best sales rank: \" + str(niches[niche]['best_sales_rank']) + \"\\n\")\n",
    "            for document in niches[niche]['similar_docs']:\n",
    "                f.write(document_data[document]['asin'] + ', ')\n",
    "            f.write(\"\\n\")\n",
    "            for document in niches[niche]['similar_docs']:\n",
    "                f.write(document_data[document]['document'] + \"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:niche_finder] *",
   "language": "python",
   "name": "conda-env-niche_finder-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
