{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['i', 'love', 'machine', 'learning', '.', 'its', 'awesome', '.'], tags=['0']), TaggedDocument(words=['i', 'love', 'coding', 'in', 'python'], tags=['1']), TaggedDocument(words=['i', 'love', 'building', 'chatbots'], tags=['2']), TaggedDocument(words=['they', 'chat', 'amagingly', 'well'], tags=['3'])]\n"
     ]
    }
   ],
   "source": [
    "data = [\"I love machine learning. Its awesome shirt.\",\n",
    "        \"I love coding in python shirt\",\n",
    "        \"I love building chatbots shirt\",\n",
    "        \"they chat amagingly well shirt\",\n",
    "        \"this means nothing\",\n",
    "        \"this means everything shirt\"]\n",
    "\n",
    "\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
    "print(test_tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-7d3cb0672a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdocuments_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdocuments_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_shirt_data_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shirts_featured_small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'B07L78RJMD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-7d3cb0672a3a>\u001b[0m in \u001b[0;36mread_shirt_data_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdocuments_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def clean_document(document):\n",
    "    document = document.lower()\n",
    "    default_1 = \"Lightweight, Classic fit, Double-needle sleeve and bottom hem\".lower()\n",
    "    default_2 = \"Solid colors: 100% Cotton; Heather Grey: 90% Cotton, 10% Polyester; All Other Heathers: 50% Cotton, 50% Polyester Imported Machine wash cold with like colors, dry low heat \".lower()\n",
    "    document = document.replace(default_1, '').replace(default_2,'')\n",
    "    \n",
    "    document.replace(\"tee shirt\",\"tshirt\")\n",
    "    document.replace(\"t-shirt\", 'tshirt')\n",
    "    document.replace(' t shirt', 'tshirt')\n",
    "    document.replace(\"-\", \" \")\n",
    "    \n",
    "    document.replace(\"shirt\", \"\")\n",
    "    document.replace(\"tshirt\", \"\")\n",
    "    \n",
    "    return document\n",
    "\n",
    "def read_shirt_data_file(path):\n",
    "    documents = []\n",
    "    asins = []\n",
    "    documents_dict = {}\n",
    "    with open(path, 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.split('|')\n",
    "            title = line[2].split(\":\",1)[1]\n",
    "\n",
    "            description = line[10].split(\":\",1)[1]\n",
    "#             print(clean_document(title))\n",
    "#             print(clean_document(description))\n",
    "            document = clean_document(title) + ': ' + clean_document(description)\n",
    "            \n",
    "            asin = line[1].split(\":\")[1]\n",
    "            \n",
    "            documents_dict[asin] = document\n",
    "            \n",
    "    return documents_dict\n",
    "\n",
    "documents_dict = read_shirt_data_file(\"shirts_featured_small\")\n",
    "print(documents_dict['B07L78RJMD'])\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(document.lower()), tags=[str(asin)]) for asin, document in documents_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/griff/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B07NYZDT3S', 0.7201009392738342), ('B07N21DZP9', 0.6711130142211914), ('B07Q96TKHF', 0.6690714955329895), ('B079ZBV4N2', 0.6659002304077148), ('B07KVJPFNW', 0.6449306011199951), ('B07K8FVYXL', 0.6160188913345337), ('B07Q4NX2X9', 0.6141048669815063), ('B079F9KT5D', 0.6111611127853394), ('B07H1N95LS', 0.6091857552528381), ('B07KVK9CL4', 0.5965323448181152)]\n",
      "autism awareness shirts autism heart shirt autism shirts: autism awareness shirts, autism awareness products, autism shirt, autism pin, autism bracelet, autism leggings, autism awareness tshirt, autism awareness wristbands, autism awareness ribbon, autism awareness shirts women, autism awareness shirts youth autism shirt mom, autism shirt kids, autism shirt brother, autism shirt girl, autistic clothes, autism tshirts for women, autism t shirt for men, autism tie, autism t shirts for women, autism tees, autism tshirt, autism tshirts for grandparents \n",
      "\n",
      "0.7005 , B07NYZDT3S - dabbing unicorn puzzle ribbon autism awareness t shirt: dabbing unicorn puzzle ribbon autism awareness t shirt \n",
      "\n",
      "0.6544 , B07K8FVYXL - sunflower accept understand love autism awareness tshirt: \n",
      "\n",
      "0.6521 , B07Q7F8H8V - jeff dunham alexandria, la shirt: \n",
      "\n",
      "0.648 , B07N21DZP9 - llama autism awareness shirt boy girl kid to be different: \n",
      "\n",
      "0.6281 , B079F9KT5D - sleketon dabbing autism awareness dare to be yourself tshirt: autism shirts women, autism shirts men, autism shirts for kids, autism shirts teacher, autism shirts dad, autism shirts for mom, autism shirts for grandpa, autism awareness shirt, autism strong tshirt. autism awareness t-shirts are a great way to show love and support acceptance. \n",
      "\n",
      "0.625 , B07Q82XDNK - rip+nipsey-+hussle shirt for men women youth: rip+nipsey-+hussle shirt for men women youth \n",
      "\n",
      "0.6155 , B07GY8FFLK - it's ok to be different autism awareness unicorn gift shirt: \n",
      "\n",
      "0.6135 , B079L93G4M - mama bear autism awareness t-shirt autism mom,mommy tee: autism shirts women, autism shirts men, autism shirts for kids, autism shirts teacher, autism shirts dad, autism shirts for mom, autism shirts for grandpa, autism awareness shirt, autism strong tshirt. autism awareness t-shirts are a great way to show love and support acceptance. \n",
      "\n",
      "0.6034 , B07Q96TKHF - texas-tech-final-four-t shirt: \n",
      "\n",
      "0.5945 , B07L78RJMD - marvel avengers endgame movie logo graphic t-shirt: officially licensed marvel tee shirt 18marf00471a \n",
      "\n",
      "0.5886 , B07H599G4Y - i like it kinky no lye - natural hair products tshirt: great ethnic tshirt for black women or african american women who embrace their kinks and curls. african american woman with natural hair - natural hair afro tees \n",
      "\n",
      "0.5834 , B07H4DBVND - jurassic park distressed vintage logo graphic t-shirt: 17jprk00308a-002 officially licensed jurassic park apparel \n",
      "\n",
      "0.5623 , B07Q4NX2X9 - nipsey-hussle-shirt: nipsey-hussle-shirt nipsey-hussle-t-shirt \n",
      "\n",
      "0.5514 , B07NYSPS5F - puzzles piece of heart love autism awareness t-shirt: autism puzzle heartbeat , heart love autism , i love someone with autism ,love autism awareness , proud autism father , mom , daddy , dad , sister , mama , papa , brother , grandma , grandpa , teacher , nurse , ninja ,principal , est 2019 ,teach , school autism awareness day , love someone with autism ,embrace different autism ,different not less ,it's ok to be different autism awareness , shirts , t-shirt , tshirts , t shirt , tee-shirt , tee , shirt , puzzle piece , st patricks day , father day \n",
      "\n",
      "0.5417 , B07N4NF233 - marvel avengers endgame movie quote graphic t-shirt: officially licensed marvel avengers endgame tee shirt 18marf00758a \n",
      "\n",
      "0.5192 , B07KWQNJVQ - autism awareness elephant t shirt: one of the best autism awareness design. this funny autism awareness elephant t-shirt is perfect for elephant lovers, toddlers, kids, mom, dad, men, women, grandpa, grandma, brother, girls or teachers. if you are looking for autism t shirts, autism awareness elephant t shirt, elephant shirts, autism apparel, autism mom tees, autism speaks t shirts, autism awareness day tshirts or autistic t shirts then this cute autism shirt is just perfect for you! \n",
      "\n",
      "0.5044 , B07N8X6TJ3 - irish shamrock boobs saint st.patrick's paddys day t-shirt: our irish shamrock boobs shirt, leprechaun shirts,ireland green day feast tshirt,four leaf clover lucky march t-shirt,irish shamrocked blood flag pride tees,celtic cross,shenanigans,gaelic shenanigator,whiskey,bourbon,beer,bar,pub,drinking,drink. men,women,him,her,boyfriend,girlfriend,friend gift for saint st st.patrick's,pat,patricks,paddys,paddy's,pattys,paddy,patties,pats day party,birthday,parties,festival celebrating,parade.(mug,hat,hoodie,sticker,decorations,earings,costume,banner,headband). \n",
      "\n",
      "[ 1.1519928  -0.6585879  -0.6154762   0.2878795  -0.7706836  -0.235372\n",
      " -0.11474528  0.37767458 -0.3943414  -0.32747498 -0.34259617  0.07763617\n",
      " -0.05985829  0.09350486  0.38040212 -0.3118476   0.23176134  0.15727162\n",
      "  0.0892503  -0.57110596  1.3840662  -0.298404   -0.4827873  -0.8562676\n",
      "  0.28215018 -0.10033531  0.5031527  -0.5618571   0.04541327  0.6673605\n",
      " -0.8718598  -0.608266    0.9929     -0.3225618  -0.38800216  0.34760085\n",
      " -0.45449215  0.65979403  0.82573855  0.6158139  -0.20410176 -0.3798712\n",
      "  0.08077389  0.17783034 -0.24937183  0.00592235 -0.1190253   0.6939613\n",
      " -1.0585874   0.7093268   0.68917805  1.1688681  -0.50503117  0.06637089\n",
      " -0.11011089  0.587007    0.13787456  0.00908433 -1.1086332  -0.3744831\n",
      "  0.8155327  -0.9764228   0.18447909 -0.28399807  0.28107852  0.29186624\n",
      "  0.45005426 -0.09327609  0.2254921   0.495531   -0.33109453 -0.01037479\n",
      "  0.9216041  -0.39253995  0.8246333   0.5409307  -0.04592625 -0.5725371\n",
      " -0.43069443  0.3901665  -0.7160472   0.24252222 -0.1039371  -0.08655967\n",
      "  1.1236031   0.16877946  0.5898015   0.5840479  -0.8951326  -0.213081\n",
      " -0.14235076 -0.2967715  -0.27968943  0.9538995  -0.5477715  -0.03135914\n",
      "  0.49675623 -0.5689562  -0.06112124  0.27607137]\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"autism awareness shirts autism heart shirt autism shirts: autism awareness shirts, autism awareness products, autism shirt, autism pin, autism bracelet, autism leggings, autism awareness tshirt, autism awareness wristbands, autism awareness ribbon, autism awareness shirts women, autism awareness shirts youth autism shirt mom, autism shirt kids, autism shirt brother, autism shirt girl, autistic clothes, autism tshirts for women, autism t shirt for men, autism tie, autism t shirts for women, autism tees, autism tshirt, autism tshirts for grandparents\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(model.docvecs.most_similar([v1]))\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = [x for x in model.docvecs.most_similar('B079ZBV4N2', topn = 50) if x[1] > .5]\n",
    "print(documents_dict['B079ZBV4N2'])\n",
    "\n",
    "for doc in similar_doc:\n",
    "    print(\"{} , {} - {}\".format(round(doc[1],4), doc[0], documents_dict[doc[0]]))\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['B07L78RJMD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texas tech final four t . \n",
      "\n",
      "Jeff Dunham Alexandria, LA Shirt. \n",
      "\n",
      "nipsey hussle shirt. nipsey hussle shirt nipsey hussle t shirt \n",
      "\n",
      "Captain Marvel Movie Goose Galaxy Portrait Graphic T Shirt. Officially Licensed Marvel Tee Shirt 18MARF00314A \n",
      "\n",
      "Captain Marvel Goose Full Portrait Space Graphic T Shirt. Officially Licensed Captain Marvel Tee Shirt 18MARF00741A \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents_dict['B07Q96TKHF'])\n",
    "print(documents_dict['B07Q7F8H8V'])\n",
    "print(documents_dict['B07Q4NX2X9'])\n",
    "print(documents_dict['B07MRD45W6'])\n",
    "print(documents_dict['B07N3GGKZ7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0244235  -0.21613473  0.18023567 -0.84176147  0.03353677  0.13153487\n",
      " -0.54832053 -0.44007263 -0.33301008  0.2449995   0.8136312   0.10832854\n",
      "  0.03200603 -0.16587837  0.6732452   0.26271158 -0.7371211  -0.38190815\n",
      "  0.53971565 -0.20095131]\n",
      "[('5', 0.9935817122459412), ('3', 0.9903445243835449), ('0', 0.9892727136611938), ('1', 0.9883068203926086), ('2', 0.985320508480072)]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs['1'])\n",
    "print(model.docvecs.most_similar('4', topn = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08148092 -0.18165147  0.07571121 -0.39440125 -0.01027174 -0.07501866\n",
      " -0.14441006 -0.24962953 -0.36294243  0.10393118  0.57642084 -0.1077453\n",
      " -0.11577217 -0.22908953  0.3654398   0.10156447 -0.35140935 -0.2015962\n",
      "  0.35865697 -0.01273198]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:niche_finder] *",
   "language": "python",
   "name": "conda-env-niche_finder-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
